{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07462edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/AItwin/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2) Imports & Configuration\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import wave\n",
    "import contextlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import re\n",
    "\n",
    "from dataclasses import dataclass, asdict,field\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f6e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running on device: mps\n",
      "[INFO] NLP mode: spacy\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Determinism (important for calibration)\n",
    "# --------------------------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Optional ML Dependencies\n",
    "# --------------------------------------------\n",
    "\n",
    "WhisperModel = None\n",
    "pipeline = None\n",
    "nlp = None\n",
    "\n",
    "# Faster-Whisper (Speech ‚Üí Text)\n",
    "try:\n",
    "    from faster_whisper import WhisperModel\n",
    "except ImportError:\n",
    "    print(\"[WARN] faster-whisper not installed. Transcription will be unavailable.\")\n",
    "\n",
    "# Transformers (Sentiment / LLMs)\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except ImportError:\n",
    "    print(\"[WARN] transformers not installed. Sentiment/LLM features disabled.\")\n",
    "\n",
    "# SpaCy (Linguistic analysis)\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NLP_MODE = \"spacy\"\n",
    "except Exception:\n",
    "    nlp = None\n",
    "    NLP_MODE = \"regex\"\n",
    "    print(\"[WARN] SpaCy model not found. Falling back to regex-based analysis (lower accuracy).\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Device Configuration (auto-detect)\n",
    "# --------------------------------------------\n",
    "\n",
    "def _detect_device():\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = _detect_device()\n",
    "\n",
    "print(f\"[INFO] Running on device: {DEVICE}\")\n",
    "print(f\"[INFO] NLP mode: {NLP_MODE}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Interview Analysis Global Config\n",
    "# (Authoritative tuning source)\n",
    "# --------------------------------------------\n",
    "\n",
    "INTERVIEW_CONFIG = {\n",
    "    # Speaking pace\n",
    "    \"ideal_wpm_range\": (125, 145),\n",
    "    \"acceptable_wpm_range\": (145, 160),\n",
    "    \"hard_penalty_wpm\": 160,\n",
    "\n",
    "    # Fillers & hesitation\n",
    "    \"max_safe_fillers_per_min\": 3.0,\n",
    "    \"filler_penalty_weight\": 1.0,\n",
    "    \"max_filler_bonus\": 5,\n",
    "\n",
    "    # Hedging & confidence\n",
    "    \"hedge_penalty_weight\": 2.5,\n",
    "    \"min_confidence_score\": 40,\n",
    "\n",
    "    # Delivery (pace + rhythm + pauses)\n",
    "    \"delivery_penalty_weight\": 0.20,\n",
    "\n",
    "    # Structure-agnostic speech control\n",
    "    \"long_block_penalty\": 12,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae5ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3) Audio Utilities & Pitch Analysis\n",
    "# ============================================\n",
    "\n",
    "def load_audio_mono(path: str, sr: int = 16000):\n",
    "    \"\"\"Load audio as mono float32 at target sampling rate.\"\"\"\n",
    "    audio, orig_sr = librosa.load(path, sr=None, mono=True)\n",
    "    if orig_sr != sr:\n",
    "        audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=sr)\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "\n",
    "def analyze_pitch_dynamics(audio: np.ndarray, sr: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze pitch variation to detect monotone delivery.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            'std_semitones': float,\n",
    "            'voiced_ratio': float,\n",
    "            'monotone_score': float,   # 0 (expressive) ‚Üí 1 (very monotone)\n",
    "            'is_monotone': bool\n",
    "        }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f0, voiced_flag, _ = librosa.pyin(\n",
    "            audio,\n",
    "            fmin=librosa.note_to_hz(\"C2\"),\n",
    "            fmax=librosa.note_to_hz(\"C7\"),\n",
    "            sr=sr,\n",
    "            frame_length=2048\n",
    "        )\n",
    "\n",
    "        # Use voiced_flag for accurate voiced coverage\n",
    "        voiced_f0 = f0[voiced_flag]\n",
    "        voiced_ratio = float(np.mean(voiced_flag)) if len(voiced_flag) else 0.0\n",
    "\n",
    "        # Not enough voiced speech ‚Üí unreliable prosody signal\n",
    "        if len(voiced_f0) < 10 or voiced_ratio < 0.25:\n",
    "            return {\n",
    "                \"std_semitones\": 0.0,\n",
    "                \"voiced_ratio\": voiced_ratio,\n",
    "                \"monotone_score\": 0.0,\n",
    "                \"is_monotone\": False\n",
    "            }\n",
    "\n",
    "        # Convert Hz ‚Üí semitones (log scale, speaker-independent)\n",
    "        ref_hz = max(np.mean(voiced_f0), 1e-3)  # numerical safety\n",
    "        semitones = 12.0 * np.log2(voiced_f0 / ref_hz)\n",
    "        std_semitones = float(np.std(semitones))\n",
    "\n",
    "        # Interview-calibrated thresholds\n",
    "        MONOTONE_CENTER = 2.5   # semitones\n",
    "        MONOTONE_LIMIT = 1.8\n",
    "\n",
    "        monotone_score = np.clip(\n",
    "            (MONOTONE_CENTER - std_semitones) / MONOTONE_CENTER,\n",
    "            0.0,\n",
    "            1.0\n",
    "        )\n",
    "        is_monotone = std_semitones < MONOTONE_LIMIT\n",
    "\n",
    "        return {\n",
    "            \"std_semitones\": std_semitones,\n",
    "            \"voiced_ratio\": voiced_ratio,\n",
    "            \"monotone_score\": monotone_score,\n",
    "            \"is_monotone\": is_monotone\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Pitch analysis failed: {e}\")\n",
    "        return {\n",
    "            \"std_semitones\": 0.0,\n",
    "            \"voiced_ratio\": 0.0,\n",
    "            \"monotone_score\": 0.0,\n",
    "            \"is_monotone\": False\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a484b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4) Transcription (Faster-Whisper) ‚Äì MPS SAFE\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class TranscriptionResult:\n",
    "    text: str\n",
    "    segments: List[Dict]\n",
    "    language: str\n",
    "    duration: float\n",
    "    num_segments: int\n",
    "\n",
    "\n",
    "_WHISPER_MODEL = None  # cached model\n",
    "\n",
    "\n",
    "def transcribe_audio(\n",
    "    audio_path: str,\n",
    "    model_size: str = \"medium\"\n",
    ") -> TranscriptionResult:\n",
    "    global _WHISPER_MODEL\n",
    "\n",
    "    if WhisperModel is None:\n",
    "        raise ImportError(\"faster-whisper is not installed.\")\n",
    "\n",
    "    # IMPORTANT: faster-whisper does NOT support MPS\n",
    "    WHISPER_DEVICE = \"cuda\" if DEVICE == \"cuda\" else \"cpu\"\n",
    "\n",
    "    if WHISPER_DEVICE == \"cuda\":\n",
    "        compute_type = \"float16\"\n",
    "    else:\n",
    "        compute_type = \"int8\"\n",
    "\n",
    "    if _WHISPER_MODEL is None:\n",
    "        _WHISPER_MODEL = WhisperModel(\n",
    "            model_size,\n",
    "            device=WHISPER_DEVICE,\n",
    "            compute_type=compute_type\n",
    "        )\n",
    "\n",
    "    segments_gen, info = _WHISPER_MODEL.transcribe(\n",
    "        audio_path,\n",
    "        beam_size=5,\n",
    "        word_timestamps=True,\n",
    "        vad_filter=True\n",
    "    )\n",
    "\n",
    "    segments = list(segments_gen)\n",
    "\n",
    "    text = \"\".join(s.text for s in segments).strip()\n",
    "\n",
    "    seg_list = []\n",
    "    all_words = []\n",
    "\n",
    "    for s in segments:\n",
    "        words = []\n",
    "        for w in (s.words or []):\n",
    "            words.append({\n",
    "                \"start\": float(w.start),\n",
    "                \"end\": float(w.end),\n",
    "                \"word\": w.word\n",
    "            })\n",
    "            all_words.append(w)\n",
    "\n",
    "        seg_list.append({\n",
    "            \"start\": float(s.start),\n",
    "            \"end\": float(s.end),\n",
    "            \"text\": s.text,\n",
    "            \"words\": words\n",
    "        })\n",
    "\n",
    "    duration = (\n",
    "        float(all_words[-1].end - all_words[0].start)\n",
    "        if all_words else 0.0\n",
    "    )\n",
    "\n",
    "    return TranscriptionResult(\n",
    "        text=text,\n",
    "        segments=seg_list,\n",
    "        language=info.language,\n",
    "        duration=duration,\n",
    "        num_segments=len(segments)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d78af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5) NLP: Context-Aware Signals\n",
    "# ============================================\n",
    "\n",
    "FILLERS_SIMPLE = {\"um\", \"uh\", \"umm\", \"uhh\"}\n",
    "MULTI_FILLERS = [\"you know\", \"i mean\"]\n",
    "\n",
    "HEDGE_KEYWORDS = {\"maybe\", \"probably\", \"possibly\", \"might\"}\n",
    "HEDGE_PHRASES = [\n",
    "    \"i think\", \"not sure\", \"kind of\", \"sort of\",\n",
    "    \"i guess\", \"might be\", \"i don't remember\"\n",
    "]\n",
    "\n",
    "OWNERSHIP_VERBS = {\"build\", \"design\", \"lead\", \"implement\", \"create\", \"manage\", \"solve\", \"drive\"}\n",
    "APOLOGIES = [\"sorry\", \"apologize\", \"i forgot\", \"i didn't prepare\", \"excuse me\"]\n",
    "\n",
    "def detect_signals(transcript: str, segments: List[Dict]) -> Dict:\n",
    "    text_lower = transcript.lower()\n",
    "\n",
    "    filler_count = 0\n",
    "    hedge_count = 0\n",
    "    own_count = 0\n",
    "    passive_count = 0\n",
    "    apology_count = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # NLP-aware analysis\n",
    "    # -------------------------\n",
    "    if nlp:\n",
    "        doc = nlp(transcript)\n",
    "\n",
    "        for token in doc:\n",
    "            t = token.text.lower()\n",
    "\n",
    "            # ---- Fillers ----\n",
    "            if t in FILLERS_SIMPLE:\n",
    "                filler_count += 1\n",
    "\n",
    "            if t == \"like\" and token.pos_ == \"INTJ\":\n",
    "                filler_count += 1\n",
    "\n",
    "            # ---- Hedging (token-level only) ----\n",
    "            if t in HEDGE_KEYWORDS:\n",
    "                hedge_count += 1\n",
    "\n",
    "            if t == \"think\" and token.head.text.lower() == \"i\":\n",
    "                hedge_count += 1\n",
    "\n",
    "            # ---- Ownership (broader patterns) ----\n",
    "            if token.lemma_ in OWNERSHIP_VERBS:\n",
    "                if any(tok.text.lower() == \"i\" for tok in token.subtree):\n",
    "                    own_count += 1\n",
    "\n",
    "            # ---- Passive (evasive only) ----\n",
    "            if token.dep_ == \"auxpass\":\n",
    "                # penalize only if no ownership nearby\n",
    "                if not any(tok.text.lower() == \"i\" for tok in token.head.subtree):\n",
    "                    passive_count += 1\n",
    "\n",
    "        # ---- Phrase-level signals (NO double count) ----\n",
    "        for f in MULTI_FILLERS:\n",
    "            filler_count += text_lower.count(f)\n",
    "\n",
    "        for h in HEDGE_PHRASES:\n",
    "            hedge_count += text_lower.count(h)\n",
    "\n",
    "    else:\n",
    "        # Regex fallback\n",
    "        filler_count += sum(text_lower.count(f) for f in FILLERS_SIMPLE)\n",
    "        filler_count += sum(text_lower.count(f) for f in MULTI_FILLERS)\n",
    "        hedge_count += sum(text_lower.count(h) for h in HEDGE_PHRASES)\n",
    "\n",
    "    # -------------------------\n",
    "    # Apologies\n",
    "    # -------------------------\n",
    "    apology_count = sum(text_lower.count(a) for a in APOLOGIES)\n",
    "\n",
    "    # -------------------------\n",
    "    # Pause & Energy Analysis\n",
    "    # -------------------------\n",
    "    long_pauses = 0\n",
    "    long_speech_blocks = 0\n",
    "\n",
    "    all_words = []\n",
    "    for seg in segments:\n",
    "        all_words.extend(seg.get(\"words\", []))\n",
    "\n",
    "    for i in range(1, len(all_words)):\n",
    "        gap = all_words[i][\"start\"] - all_words[i-1][\"end\"]\n",
    "        if gap > 1.2:\n",
    "            long_pauses += 1\n",
    "\n",
    "    for seg in segments:\n",
    "        if (seg[\"end\"] - seg[\"start\"]) > 10.0:\n",
    "            long_speech_blocks += 1\n",
    "\n",
    "    # -------------------------\n",
    "    # Semantic uncertainty\n",
    "    # -------------------------\n",
    "    uncertainty_patterns = [\"or maybe\", \"not sure if\", \"i think it was\", \"can't remember\"]\n",
    "    hedge_count += sum(text_lower.count(p) for p in uncertainty_patterns)\n",
    "\n",
    "    return {\n",
    "        \"filler_count\": filler_count,\n",
    "        \"hedge_count\": hedge_count,\n",
    "        \"own_count\": own_count,\n",
    "        \"passive_count\": passive_count,\n",
    "        \"apology_count\": apology_count,\n",
    "        \"long_pauses\": long_pauses,\n",
    "        \"long_speech_blocks\": long_speech_blocks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4250c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6) Advanced Scoring Engine (Calibrated)\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class InterviewScore:\n",
    "    total_score: float\n",
    "    metrics: Dict\n",
    "    feedback: List[str]\n",
    "\n",
    "\n",
    "def calculate_score(\n",
    "    transcript: str,\n",
    "    duration: float,\n",
    "    signals: Dict,\n",
    "    pitch_data: Dict,\n",
    "    sentiment_res\n",
    ") -> InterviewScore:\n",
    "\n",
    "    score = 100.0\n",
    "    feedback = []\n",
    "\n",
    "    duration_min = max(duration / 60.0, 1.0)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. CONFIDENCE (HEDGING)\n",
    "    # -------------------------\n",
    "    hedges_per_min = signals[\"hedge_count\"] / duration_min\n",
    "    if hedges_per_min > 1.0:\n",
    "        pen = min((hedges_per_min - 1.0) * 4.0, 22.0)\n",
    "        score -= pen\n",
    "        feedback.append(\n",
    "            f\"Hedging detected ({hedges_per_min:.1f}/min). Be more decisive.\"\n",
    "        )\n",
    "\n",
    "    if signals[\"apology_count\"] > 0:\n",
    "        pen = signals[\"apology_count\"] * 4.0\n",
    "        score -= pen\n",
    "        feedback.append(\"Avoid apologizing or underselling yourself.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. OWNERSHIP vs PASSIVE\n",
    "    # -------------------------\n",
    "    own_rate = signals[\"own_count\"] / duration_min\n",
    "    passive_rate = signals[\"passive_count\"] / duration_min\n",
    "\n",
    "    if own_rate > passive_rate + 0.5:\n",
    "        bonus = min((own_rate - passive_rate) * 2.0, 8.0)\n",
    "        score += bonus\n",
    "        feedback.append(\"Good ownership language detected.\")\n",
    "    elif passive_rate > own_rate + 1.0:\n",
    "        score -= 5.0\n",
    "        feedback.append(\"Excessive passive voice. Use active language.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. DELIVERY\n",
    "    # -------------------------\n",
    "\n",
    "    # A) Fillers\n",
    "    fillers_per_min = signals[\"filler_count\"] / duration_min\n",
    "    if fillers_per_min > 3.0:\n",
    "        pen = min((fillers_per_min - 3.0) * 2.0, 15.0)\n",
    "        score -= pen\n",
    "        feedback.append(\n",
    "            f\"High filler usage ({fillers_per_min:.1f}/min).\"\n",
    "        )\n",
    "\n",
    "    # B) Long pauses\n",
    "    pauses_per_min = signals[\"long_pauses\"] / duration_min\n",
    "    if pauses_per_min > 2.0:\n",
    "        pen = min((pauses_per_min - 2.0) * 1.5, 8.0)\n",
    "        score -= pen\n",
    "        feedback.append(\"Frequent long pauses detected.\")\n",
    "\n",
    "    # C) WPM (clearer penalty)\n",
    "    wpm = (len(transcript.split()) / duration) * 60 if duration > 0 else 0\n",
    "\n",
    "    if wpm < 115:\n",
    "        score -= min((115 - wpm) * 0.2, 10.0)\n",
    "        feedback.append(f\"Pace is slow ({wpm:.0f} WPM).\")\n",
    "    elif wpm > 155:\n",
    "        score -= min((wpm - 155) * 0.4, 15.0)\n",
    "        feedback.append(f\"Pace is fast ({wpm:.0f} WPM). Slow down.\")\n",
    "\n",
    "    # D) Energy consistency\n",
    "    if signals[\"long_speech_blocks\"] > 0:\n",
    "        pen = min(signals[\"long_speech_blocks\"] * 4.0, 10.0)\n",
    "        score -= pen\n",
    "        feedback.append(\"Break long explanations with pauses.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 4. VOICE MODULATION\n",
    "    # -------------------------\n",
    "    monotone_score = pitch_data.get(\"monotone_score\", 0.0)\n",
    "    if monotone_score > 0.6:\n",
    "        pen = monotone_score * 8.0\n",
    "        score -= pen\n",
    "        feedback.append(\"Voice sounds monotone. Add variation.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 5. SENTIMENT (POLISH ONLY)\n",
    "    # -------------------------\n",
    "    if sentiment_res:\n",
    "        label = sentiment_res[0][\"label\"]\n",
    "        conf = sentiment_res[0][\"score\"]\n",
    "\n",
    "        if label == \"POSITIVE\" and conf > 0.9:\n",
    "            score += 1.5\n",
    "            feedback.append(\"Positive tone.\")\n",
    "        elif label == \"NEGATIVE\" and conf > 0.9:\n",
    "            score -= 5.0\n",
    "            feedback.append(\"Tone sounds uncertain.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 6. CONFIDENCE CEILING\n",
    "    # -------------------------\n",
    "    # High hedging should cap final score\n",
    "    if hedges_per_min > 2.0:\n",
    "        score = min(score, 78.0)\n",
    "\n",
    "    # -------------------------\n",
    "    # Final clamp\n",
    "    # -------------------------\n",
    "    score = max(0.0, min(100.0, score))\n",
    "    # Absolute realism cap\n",
    "    if score > 95:\n",
    "        score = 95.0\n",
    "\n",
    "\n",
    "    return InterviewScore(\n",
    "        total_score=score,\n",
    "        metrics={\n",
    "            **signals,\n",
    "            \"wpm\": wpm,\n",
    "            \"fillers_per_min\": fillers_per_min,\n",
    "            \"monotone_score\": monotone_score,\n",
    "        },\n",
    "        feedback=feedback,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c22952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7) Main Runner (Final)\n",
    "# ============================================\n",
    "\n",
    "_SENT_PIPE = None  # lazy-loaded sentiment pipeline\n",
    "_PIPELINE_AVAILABLE = pipeline is not None\n",
    "\n",
    "\n",
    "def _sample_text_for_sentiment(text: str, max_len: int = 512) -> str:\n",
    "    \"\"\"Sample beginning + middle + end for fair sentiment.\"\"\"\n",
    "    if len(text) <= max_len:\n",
    "        return text\n",
    "\n",
    "    part = max_len // 3\n",
    "    return (\n",
    "        text[:part] +\n",
    "        text[len(text)//2 - part//2 : len(text)//2 + part//2] +\n",
    "        text[-part:]\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_interview(audio_path: str):\n",
    "    print(f\"--- Analyzing {os.path.basename(audio_path)} ---\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. Load Audio & Pitch\n",
    "    # -------------------------\n",
    "    print(\"Loading audio...\")\n",
    "    audio, sr = load_audio_mono(audio_path)\n",
    "    pitch_data = analyze_pitch_dynamics(audio, sr)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. Transcription\n",
    "    # -------------------------\n",
    "    print(\"Transcribing...\")\n",
    "    tr = transcribe_audio(audio_path, model_size=\"medium\")\n",
    "\n",
    "    if tr is None:\n",
    "        print(\"[ERROR] Transcription failed. Whisper returned None.\")\n",
    "        return None\n",
    "\n",
    "    if not tr.text or not tr.text.strip():\n",
    "        print(\"[ERROR] Empty transcription. Audio may be silent or corrupted.\")\n",
    "        return None\n",
    "\n",
    "    # print(\"\\n--- FULL TRANSCRIPT ---\")\n",
    "    # print(tr.text)\n",
    "    # print(\"--- END TRANSCRIPT ---\\n\")\n",
    "\n",
    "    # Use effective spoken duration\n",
    "    duration = tr.duration if tr.duration > 0 else len(audio) / sr\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. Linguistic Signals\n",
    "    # -------------------------\n",
    "    print(\"Analyzing linguistic signals...\")\n",
    "    signals = detect_signals(tr.text, tr.segments)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4. Sentiment (Optional)\n",
    "    # -------------------------\n",
    "    sent_res = None\n",
    "    if _PIPELINE_AVAILABLE:\n",
    "        global _SENT_PIPE\n",
    "        if _SENT_PIPE is None:\n",
    "            _SENT_PIPE = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                device=-1\n",
    "            )\n",
    "\n",
    "        sent_text = _sample_text_for_sentiment(tr.text)\n",
    "        sent_res = _SENT_PIPE(sent_text)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5. CS Score\n",
    "    # -------------------------\n",
    "    result = calculate_score(\n",
    "        transcript=tr.text,\n",
    "        duration=duration,\n",
    "        signals=signals,\n",
    "        pitch_data=pitch_data,\n",
    "        sentiment_res=sent_res\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 6. Report (unchanged)\n",
    "    # -------------------------\n",
    "    print(\"\\n=== INTERVIEW REPORT ===\")\n",
    "    print(f\"OVERALL SCORE: {result.total_score:.1f} / 100\")\n",
    "\n",
    "    print(\"\\n--- Feedback ---\")\n",
    "    for f in result.feedback:\n",
    "        print(f\"[ ] {f}\")\n",
    "\n",
    "    print(\"\\n--- Detailed Metrics ---\")\n",
    "    for k, v in result.metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k}: {v:.2f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n--- Debug Info ---\")\n",
    "    print(f\"NLP mode: {NLP_MODE}\")\n",
    "    print(f\"Speech duration (s): {duration:.2f}\")\n",
    "    print(f\"Voiced ratio: {pitch_data.get('voiced_ratio', 'NA')}\")\n",
    "\n",
    "    # =========================\n",
    "    # ‚úÖ RETURN VALUES FOR TCS\n",
    "    # =========================\n",
    "    return {\n",
    "        \"transcript\": tr.text,\n",
    "        \"cs_score\": result.total_score,\n",
    "        \"cs_result\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181b9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e4300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c5fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TCS Model Configuration\n",
    "# -----------------------------\n",
    "TCS_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN is None:\n",
    "    raise RuntimeError(\"HF_TOKEN environment variable not set\")\n",
    "\n",
    "# ---- Tokenizer ----\n",
    "_tcs_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TCS_MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "if _tcs_tokenizer.pad_token is None:\n",
    "    _tcs_tokenizer.pad_token = _tcs_tokenizer.eos_token\n",
    "\n",
    "# ---- Device & dtype ----\n",
    "if DEVICE == \"mps\":\n",
    "    _tcs_dtype = torch.float32\n",
    "    _device_map = None\n",
    "elif DEVICE == \"cuda\":\n",
    "    _tcs_dtype = torch.float16\n",
    "    _device_map = \"auto\"\n",
    "else:\n",
    "    _tcs_dtype = torch.float32\n",
    "    _device_map = None\n",
    "\n",
    "# ---- Model ----\n",
    "_tcs_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TCS_MODEL_NAME,\n",
    "    torch_dtype=_tcs_dtype,\n",
    "    device_map=_device_map,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "_tcs_model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e00cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TechnicalEvaluationResult:\n",
    "    score: int\n",
    "    band: str\n",
    "    verdict: str\n",
    "\n",
    "    issues: List[str] = field(default_factory=list)\n",
    "    improvement_points: List[str] = field(default_factory=list)\n",
    "\n",
    "    conceptual_score: int | None = None\n",
    "    specificity_score: int | None = None\n",
    "    confidence_score: int | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888c35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def build_tcs_prompt(question: str | List[str], transcript: str) -> str:\n",
    "    # Normalize question in case a list/array is passed\n",
    "    if isinstance(question, list):\n",
    "        question = next(\n",
    "            (str(q).strip() for q in question if str(q).strip()),\n",
    "            \"Explain your approach to this problem.\"\n",
    "        )\n",
    "    else:\n",
    "        question = str(question).strip() or \"Explain your approach to this problem.\"\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a senior technical interviewer conducting a mock interview.\n",
    "\n",
    "You must evaluate the candidate STRICTLY based on:\n",
    "1. The interview question provided\n",
    "2. The candidate‚Äôs answer provided below\n",
    "\n",
    "You have NO access to the candidate‚Äôs resume, background, or intent beyond\n",
    "what is explicitly stated.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Determine whether the candidate actually answered the question asked.\n",
    "2. Evaluate technical correctness ONLY within the scope of the question.\n",
    "3. Identify inaccuracies, misconceptions, missing fundamentals, or weak\n",
    "   explanations relative to the question.\n",
    "4. Provide transcript-grounded coaching feedback to improve correctness,\n",
    "   relevance, and clarity.\n",
    "\n",
    "STRICT EVALUATION RULES:\n",
    "- Judge relevance: Penalize if the answer partially or fully misses the question.\n",
    "- Judge correctness: Evaluate only what the candidate actually said.\n",
    "- Do NOT infer unstated knowledge or intentions.\n",
    "- Do NOT introduce new tools, technologies, metrics, or concepts.\n",
    "- Do NOT penalize for advanced topics unless the question explicitly requires them.\n",
    "- Avoid generic interview advice (e.g., ‚Äúpractice more‚Äù, ‚Äúbe confident‚Äù).\n",
    "\n",
    "Interview Question:\n",
    "{question}\n",
    "\n",
    "Candidate Answer:\n",
    "{transcript}\n",
    "\n",
    "SCORING GUIDELINES:\n",
    "- Score from 0 to 100 based on relevance + technical correctness.\n",
    "- Use these bands:\n",
    "  - Excellent: Fully answers the question with correct and clear explanation\n",
    "  - Good: Answers the question correctly with minor gaps or imprecision\n",
    "  - Partial: Addresses the question but with notable gaps or confusion\n",
    "  - Weak: Poor alignment with the question or flawed understanding\n",
    "  - Poor: Does not answer the question or is mostly incorrect\n",
    "\n",
    "COACHING REQUIREMENTS:\n",
    "- Provide at least 4 coaching points.\n",
    "- EACH coaching point must reference:\n",
    "  - something said in the answer, OR\n",
    "  - something clearly missing relative to the question.\n",
    "- If the answer is strong, focus on improving precision, structure, or depth\n",
    "  without adding new content.\n",
    "\n",
    "OUTPUT RULES:\n",
    "- Start the response with '{{' and end with '}}'.\n",
    "- Respond in STRICT JSON ONLY.\n",
    "- Do NOT include markdown, explanations, or extra text.\n",
    "\n",
    "JSON format:\n",
    "{{\n",
    "  \"score\": <int>,\n",
    "  \"band\": \"<Excellent|Good|Partial|Weak|Poor>\",\n",
    "  \"verdict\": \"<1‚Äì2 sentence technical summary judging alignment with the question>\",\n",
    "  \"issues\": [\"<question-relative technical issues or 'No major technical issues identified'>\"],\n",
    "  \"improvement_points\": [\"<specific, question-grounded coaching points>\"]\n",
    "}}\n",
    "\n",
    "Return only valid JSON.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6be8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_last_json(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the LAST valid JSON object from LLM output.\n",
    "    Handles multiple JSON blocks safely.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r\"\\{[\\s\\S]*?\\}\", text)\n",
    "    if not matches:\n",
    "        raise RuntimeError(\"No JSON object found in LLM output\")\n",
    "\n",
    "    for candidate in reversed(matches):\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"No valid JSON object could be parsed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d42582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_valid_json_objects(text: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extracts ALL fully balanced JSON objects from text\n",
    "    and returns them as parsed dicts.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    stack = []\n",
    "    start = None\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if ch == \"{\":\n",
    "            if not stack:\n",
    "                start = i\n",
    "            stack.append(\"{\")\n",
    "\n",
    "        elif ch == \"}\":\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack and start is not None:\n",
    "                    candidate = text[start:i+1]\n",
    "                    try:\n",
    "                        results.append(json.loads(candidate))\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                    start = None\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca4da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_llm(prompt: str, max_new_tokens: int = 1600) -> dict:\n",
    "\n",
    "    inputs = _tcs_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2536\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(_tcs_model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = _tcs_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=_tcs_tokenizer.eos_token_id,\n",
    "            pad_token_id=_tcs_tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    print(\"[TCS] Model response received. Parsing output...\")\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    decoded = _tcs_tokenizer.decode(\n",
    "        outputs[0][input_len:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    # üî• ROBUST JSON EXTRACTION\n",
    "    parsed_objects = extract_valid_json_objects(decoded)\n",
    "\n",
    "    if parsed_objects:\n",
    "        return parsed_objects[-1]  # ‚úÖ LAST FULL VALID JSON\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"TCS LLM output could not be parsed into valid JSON.\\n\"\n",
    "        \"Raw output:\\n\" + decoded[-1000:]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c4a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_tcs(score: int) -> str:\n",
    "    if score >= 85:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 75:\n",
    "        return \"Good\"\n",
    "    elif score >= 60:\n",
    "        return \"Partial\"\n",
    "    elif score >= 35:\n",
    "        return \"Weak\"\n",
    "    else:\n",
    "        return \"Poor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20430b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d765277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QuestionGenerationRequest:\n",
    "    role: str\n",
    "    experience: str\n",
    "    company_type: str\n",
    "    interview_round: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30eb745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_question_generation_prompt(req: QuestionGenerationRequest) -> str:\n",
    "    return f\"\"\"\n",
    "You are a professional interviewer.\n",
    "\n",
    "Interview Context:\n",
    "- Role: {req.role}\n",
    "- Experience Level: {req.experience}\n",
    "- Company Type: {req.company_type}\n",
    "- Interview Round: {req.interview_round}\n",
    "\n",
    "QUESTION COUNT RULES:\n",
    "- Ask questions based on the Experience Level ,Role,Company Type , Interview Round.\n",
    "- HR Round: exactly 6 questions\n",
    "- Technical Round: exactly 8 questions\n",
    "- DSA Round: exactly 7 questions\n",
    "- Coding Round: exactly 5 questions\n",
    "- Communication Round: exactly 5 questions\n",
    "\n",
    "\n",
    "CRITICAL OUTPUT CONTRACT:\n",
    "- Output ONLY one valid JSON object.\n",
    "- Do NOT add any text before or after the JSON.\n",
    "- Do NOT add explanations, notes, labels, or headings.\n",
    "- Stop generating immediately after the final closing brace.\n",
    "- If any extra text is added, the output is INVALID.\n",
    "\n",
    "MANDATORY JSON FORMAT:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    \"question_1\",\n",
    "    \"question_2\",\n",
    "    \"question_3\",\n",
    "    \"question_4\",\n",
    "    \"question_5\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- The number of questions in the array MUST exactly match the rule for the selected Interview Round.\n",
    "\n",
    "Return the JSON now and stop.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6dbfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_question(prompt: str, max_new_tokens: int = 512) -> Dict:\n",
    "    def _fix_and_load(block: str) -> Dict:\n",
    "        cleaned = re.sub(r\",\\s*\\}\", \"}\", block)\n",
    "        cleaned = re.sub(r\",\\s*\\]\", \"]\", cleaned)\n",
    "        bracket_diff = cleaned.count(\"[\") - cleaned.count(\"]\")\n",
    "        brace_diff = cleaned.count(\"{\") - cleaned.count(\"}\")\n",
    "        if bracket_diff > 0:\n",
    "            cleaned += \"]\" * bracket_diff\n",
    "        if brace_diff > 0:\n",
    "            cleaned += \"}\" * brace_diff\n",
    "        return json.loads(cleaned)\n",
    "\n",
    "    inputs = _tcs_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(_tcs_model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = _tcs_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=_tcs_tokenizer.eos_token_id,\n",
    "            pad_token_id=_tcs_tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only newly generated tokens\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    decoded = _tcs_tokenizer.decode(\n",
    "        outputs[0][input_len:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    # Defensive JSON extraction: prefer last valid JSON block, clean trailing commas, balance braces\n",
    "    json_blocks = re.findall(r\"\\{[\\s\\S]*?\\}\", decoded)\n",
    "    for block in reversed(json_blocks):\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                return _fix_and_load(block)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # If no block parsed, try from first brace onward\n",
    "    if \"{\" in decoded:\n",
    "        tail = decoded[decoded.find(\"{\") :]\n",
    "        try:\n",
    "            return _fix_and_load(tail)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Question LLM returned invalid JSON.\\nRaw output:\\n\" + decoded\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32950e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def generate_interview_questions(req: QuestionGenerationRequest) -> List[str]:\n",
    "    response: Dict = run_llm_question(\n",
    "        build_question_generation_prompt(req),\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    if \"questions\" not in response:\n",
    "        raise RuntimeError(\"LLM response missing 'questions' field\")\n",
    "\n",
    "    if not isinstance(response[\"questions\"], list):\n",
    "        raise RuntimeError(\"'questions' must be a list\")\n",
    "\n",
    "    questions = [str(q).strip() for q in response[\"questions\"] if str(q).strip()]\n",
    "\n",
    "    expected_counts = {\n",
    "        \"HR\": 6,\n",
    "        \"Technical\": 8,\n",
    "        \"DSA\": 7,\n",
    "        \"Coding\": 5,\n",
    "        \"Communication\": 5,\n",
    "    }\n",
    "    expected = expected_counts.get(req.interview_round)\n",
    "\n",
    "    if expected:\n",
    "        if len(questions) > expected:\n",
    "            questions = questions[:expected]\n",
    "        elif len(questions) < expected:\n",
    "            raise RuntimeError(\n",
    "                f\"Expected {expected} questions for {req.interview_round} round, got {len(questions)}. Raw: {response}\"\n",
    "            )\n",
    "\n",
    "    return questions\n",
    "\n",
    "\n",
    "def generate_interview_question(req: QuestionGenerationRequest) -> str:\n",
    "    questions = generate_interview_questions(req)\n",
    "    if not questions:\n",
    "        raise RuntimeError(\"No questions returned by LLM\")\n",
    "    return questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a208cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "1. Can you tell us about your academic background?\n",
      "2. What inspired you to pursue a career in software development?\n",
      "3. How do you stay updated with the latest technologies and trends in the field?\n",
      "4. Can you walk us through your resume and highlight your relevant skills?\n",
      "5. What are your greatest strengths and weaknesses as a software developer?\n",
      "6. Why do you want to work with our company?\n"
     ]
    }
   ],
   "source": [
    "req = QuestionGenerationRequest(\n",
    "    role=\"Software Development Engineer\",\n",
    "    experience=\"Fresher\",\n",
    "    company_type=\"Service-Based\",\n",
    "    interview_round=\"HR\"\n",
    ")\n",
    "\n",
    "questions = generate_interview_questions(req)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e6362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# req = QuestionGenerationRequest(\n",
    "#     role=\"Marketing Manager\",\n",
    "#     experience=\"1-3 years\",\n",
    "#     company_type=\"Product-Based\",\n",
    "#     interview_round=\"Technical Interview\"\n",
    "# )\n",
    "\n",
    "# questions = generate_interview_questions(req)\n",
    "\n",
    "# print(\"Generated Questions:\")\n",
    "# for i, q in enumerate(questions, 1):\n",
    "#     print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a6ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4c8009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can you tell us about your academic background?', 'What inspired you to pursue a career in software development?', 'How do you stay updated with the latest technologies and trends in the field?', 'Can you walk us through your resume and highlight your relevant skills?', 'What are your greatest strengths and weaknesses as a software developer?', 'Why do you want to work with our company?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "526beea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def run_tcs_llm(\n",
    "    transcript: str,\n",
    "    question: str | List[str] | None = None\n",
    ") -> dict:\n",
    "    print(\"[TCS] Running technical correctness evaluation...\")\n",
    "\n",
    "    # Safe default question\n",
    "    if question is None:\n",
    "        question = \"Explain your approach to this problem.\"\n",
    "\n",
    "    return _run_llm(\n",
    "        build_tcs_prompt(question, transcript),\n",
    "        max_new_tokens=1600\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ef500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tcs(transcript: str, question: str | List[str] | None = None) -> TechnicalEvaluationResult:\n",
    "    raw = run_tcs_llm(transcript, question)\n",
    "\n",
    "    # ---- Score ----\n",
    "    if \"score\" not in raw:\n",
    "        raise RuntimeError(f\"TCS output missing 'score'. Raw response: {raw}\")\n",
    "\n",
    "    score = int(raw[\"score\"])\n",
    "    score = max(0, min(score, 100))\n",
    "\n",
    "    band = bucket_tcs(score)\n",
    "    verdict = raw.get(\"verdict\", \"\").strip()\n",
    "\n",
    "    # ---- Issues ----\n",
    "    issues = raw.get(\"issues\", [])\n",
    "    if not isinstance(issues, list) or not issues:\n",
    "        issues = [\"No major technical issues identified.\"]\n",
    "\n",
    "    # ---- Coaching points ----\n",
    "    improvement_points = raw.get(\"improvement_points\", [])\n",
    "    if not isinstance(improvement_points, list) or not improvement_points:\n",
    "        improvement_points = [\n",
    "            \"Improve clarity and specificity while explaining technical decisions.\"\n",
    "        ]\n",
    "    \n",
    "    print(question)\n",
    "\n",
    "    return TechnicalEvaluationResult(\n",
    "        score=score,\n",
    "        band=band,\n",
    "        verdict=verdict,\n",
    "        issues=issues,\n",
    "        improvement_points=improvement_points\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2917bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cs_tcs(cs_score: float, tcs: TechnicalEvaluationResult) -> float:\n",
    "    # ---- Weighted fusion (technical slightly dominant) ----\n",
    "    final_score = 0.6 * cs_score + 0.4 * tcs.score\n",
    "\n",
    "    # ---- Interview realism constraints ----\n",
    "    if tcs.band == \"Poor\":\n",
    "        final_score = min(final_score, 45.0)\n",
    "    elif tcs.band == \"Weak\":\n",
    "        final_score = min(final_score, 60.0)\n",
    "    elif tcs.band == \"Partial\":\n",
    "        final_score = min(final_score, 82.0)  # slightly stricter than 85\n",
    "\n",
    "    # ---- Absolute realism bounds ----\n",
    "    final_score = min(final_score, max(cs_score, tcs.score))\n",
    "    final_score = min(final_score, 95.0)\n",
    "    final_score = max(final_score, 0.0)\n",
    "\n",
    "    return round(final_score, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fb9fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pretty_print_transcript(text: str, line_width: int = 100):\n",
    "    words = text.split()\n",
    "    line = []\n",
    "    for w in words:\n",
    "        line.append(w)\n",
    "        if sum(len(x) + 1 for x in line) >= line_width:\n",
    "            print(\" \".join(line))\n",
    "            line = []\n",
    "    if line:\n",
    "        print(\" \".join(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2790c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def build_placement_coaching_prompt(question: str | List[str], transcript: str) -> str:\n",
    "    if isinstance(question, list):\n",
    "        question = next(\n",
    "            (str(q).strip() for q in question if str(q).strip()),\n",
    "            \"Explain your approach to this problem.\"\n",
    "        )\n",
    "    else:\n",
    "        question = str(question).strip() or \"Explain your approach to this problem.\"\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a senior placement officer reviewing a mock interview response.\n",
    "\n",
    "You must evaluate the candidate STRICTLY based on:\n",
    "- The interview transcript provided below\n",
    "- Evidence explicitly present in the transcript\n",
    "\n",
    "You have NO access to the candidate‚Äôs resume, background, or intent beyond\n",
    "what is stated in the transcript.\n",
    "\n",
    "YOUR RESPONSIBILITIES:\n",
    "1. Identify concrete strengths demonstrated in the response.\n",
    "2. Identify placement-relevant weaknesses or gaps visible in the response.\n",
    "3. Provide focused coaching advice to improve placement readiness.\n",
    "\n",
    "EVALUATION RULES:\n",
    "- Base every point directly on the transcript.\n",
    "- Do NOT invent skills, experience, or achievements.\n",
    "- Do NOT add tools, technologies, or concepts not mentioned.\n",
    "- Avoid generic advice (e.g., ‚Äúpractice more‚Äù, ‚Äúbe confident‚Äù).\n",
    "- If evidence is limited, infer conservatively from what is missing.\n",
    "\n",
    "MANDATORY OUTPUT REQUIREMENTS:\n",
    "- \"standout_strengths\" MUST contain **3 to 4 distinct items**\n",
    "- \"top_improvements\" MUST contain **3 to 4 distinct items**\n",
    "- \"current_gaps\" MUST contain **at least 2 items**\n",
    "- \"actionable_improvements\" MUST contain **at least 2 items**\n",
    "- \"placement_focus\" MUST contain **at least 2 items**\n",
    "- Each item must be concise and transcript-grounded\n",
    "\n",
    "OUTPUT CONSTRAINTS:\n",
    "- Return EXACTLY ONE JSON object.\n",
    "- Start the response with '{{' and end with '}}'.\n",
    "- Output STRICT JSON only (no text, no markdown).\n",
    "- All values MUST be arrays of strings.\n",
    "- Keep each point short and specific (no long paragraphs).\n",
    "\n",
    "Interview Transcript:\n",
    "{transcript}\n",
    "\n",
    "JSON format (FOLLOW EXACTLY):\n",
    "\n",
    "{{\n",
    "  \"standout_strengths\": [\n",
    "    \"<strength 1>\",\n",
    "    \"<strength 2>\",\n",
    "    \"<strength 3>\",\n",
    "    \"<optional strength 4>\"\n",
    "  ],\n",
    "  \"top_improvements\": [\n",
    "    \"<improvement 1>\",\n",
    "    \"<improvement 2>\",\n",
    "    \"<improvement 3>\",\n",
    "    \"<optional improvement 4>\"\n",
    "  ],\n",
    "  \"placement_coaching\": {{\n",
    "    \"current_gaps\": [\n",
    "      \"<gap 1>\",\n",
    "      \"<gap 2>\"\n",
    "    ],\n",
    "    \"actionable_improvements\": [\n",
    "      \"<actionable advice 1>\",\n",
    "      \"<actionable advice 2>\"\n",
    "    ],\n",
    "    \"placement_focus\": [\n",
    "      \"<focus area 1>\",\n",
    "      \"<focus area 2>\"\n",
    "    ]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Return only valid JSON.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01aa32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def run_placement_coaching_llm(\n",
    "    transcript: str,\n",
    "    question: str | List[str] | None = None\n",
    ") -> dict:\n",
    "    print(\"[COACH] Running placement coaching...\")\n",
    "\n",
    "    prompt = build_placement_coaching_prompt(question, transcript)\n",
    "\n",
    "    try:\n",
    "        # _run_llm MUST return a valid dict\n",
    "        output = _run_llm(prompt, max_new_tokens=800)\n",
    "\n",
    "        if not isinstance(output, dict):\n",
    "            raise RuntimeError(\"LLM output is not a JSON object\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[COACH][WARN] Placement coaching failed. Using hard fallback.\")\n",
    "        print(str(e))\n",
    "\n",
    "        return {\n",
    "            \"standout_strengths\": [\n",
    "                \"Participated actively in the interview\"\n",
    "            ],\n",
    "            \"top_improvements\": [\n",
    "                \"Improve clarity and confidence in explanations\"\n",
    "            ],\n",
    "            \"placement_coaching\": {\n",
    "                \"current_gaps\": [\n",
    "                    \"Responses lack structured depth\"\n",
    "                ],\n",
    "                \"actionable_improvements\": [\n",
    "                    \"Practice explaining answers step-by-step\"\n",
    "                ],\n",
    "                \"placement_focus\": [\n",
    "                    \"Communication clarity and interview readiness\"\n",
    "                ]\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fea2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def generate_placement_feedback(\n",
    "    transcript: str,\n",
    "    question: str | List[str] | None = None\n",
    ") -> dict:\n",
    "\n",
    "    raw = run_placement_coaching_llm(transcript, question)\n",
    "\n",
    "    # ---- Safe list extraction (NON-DESTRUCTIVE) ----\n",
    "    def ensure_list(value, fallback):\n",
    "        if isinstance(value, list) and len(value) > 0:\n",
    "            return value\n",
    "        return [fallback]\n",
    "\n",
    "    standout_strengths = ensure_list(\n",
    "        raw.get(\"standout_strengths\"),\n",
    "        \"Shows basic engagement during the interview\"\n",
    "    )\n",
    "\n",
    "    top_improvements = ensure_list(\n",
    "        raw.get(\"top_improvements\"),\n",
    "        \"Needs more structured and confident explanations\"\n",
    "    )\n",
    "\n",
    "    # ---- Normalize placement coaching ----\n",
    "    placement_raw = raw.get(\"placement_coaching\", {})\n",
    "\n",
    "    placement = {\n",
    "        \"current_gaps\": ensure_list(\n",
    "            placement_raw.get(\"current_gaps\"),\n",
    "            \"Lacks depth or clarity in some responses\"\n",
    "        ),\n",
    "        \"actionable_improvements\": ensure_list(\n",
    "            placement_raw.get(\"actionable_improvements\"),\n",
    "            \"Practice explaining answers step-by-step with examples\"\n",
    "        ),\n",
    "        \"placement_focus\": ensure_list(\n",
    "            placement_raw.get(\"placement_focus\"),\n",
    "            \"Focus on communication clarity and interview readiness\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"standout_strengths\": standout_strengths,\n",
    "        \"top_improvements\": top_improvements,\n",
    "        \"placement_coaching\": placement\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024764d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "014df5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing testing_audio.wav ---\n",
      "Loading audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/AItwin/lib/python3.11/site-packages/faster_whisper/feature_extractor.py:224: RuntimeWarning: divide by zero encountered in matmul\n",
      "  mel_spec = self.mel_filters @ magnitudes\n",
      "/opt/miniconda3/envs/AItwin/lib/python3.11/site-packages/faster_whisper/feature_extractor.py:224: RuntimeWarning: overflow encountered in matmul\n",
      "  mel_spec = self.mel_filters @ magnitudes\n",
      "/opt/miniconda3/envs/AItwin/lib/python3.11/site-packages/faster_whisper/feature_extractor.py:224: RuntimeWarning: invalid value encountered in matmul\n",
      "  mel_spec = self.mel_filters @ magnitudes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing linguistic signals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INTERVIEW REPORT ===\n",
      "OVERALL SCORE: 95.0 / 100\n",
      "\n",
      "--- Feedback ---\n",
      "[ ] Good ownership language detected.\n",
      "[ ] Positive tone.\n",
      "\n",
      "--- Detailed Metrics ---\n",
      "filler_count: 0\n",
      "hedge_count: 0\n",
      "own_count: 1\n",
      "passive_count: 0\n",
      "apology_count: 0\n",
      "long_pauses: 0\n",
      "long_speech_blocks: 0\n",
      "wpm: 136.97\n",
      "fillers_per_min: 0.00\n",
      "monotone_score: 0.00\n",
      "\n",
      "--- Debug Info ---\n",
      "NLP mode: spacy\n",
      "Speech duration (s): 116.08\n",
      "Voiced ratio: 0.44386351128233353\n",
      "=== COMMUNICATION SCORE (CS) ===\n",
      "CS Score      : 95.0\n",
      "\n",
      "--- TRANSCRIPT ---\n",
      "Hello, I am Suhan Kumar. I am currently pursuing a bachelor's degree in computer science engineering\n",
      "with a specialization in artificial intelligence and machine learning. Through my coursework I have\n",
      "built a strong foundation in core computer science of subjects such as programming, data structures,\n",
      "databases and software fundamentals. Along with academics, I have applied these concepts in practical\n",
      "projects which have helped me understand how theoretical knowledge translates into real-world applications.\n",
      "I was motivated to pursue a career in software development because I enjoy problem-solving and building\n",
      "solutions that create real impact. Writing code allows me to convert ideas to working systems and this\n",
      "continuous process of learning and improvement strongly aligns with my interest. To stay updated with\n",
      "the latest technologies I regularly follow technical blogs and official documentation. And I reinforce\n",
      "my learning by working on hands-on projects. I also explore tools and frameworks that are currently\n",
      "relevant to industry needs. From my resume my primary skills lie in front-end development and problem-solving.\n",
      "I have worked with HTML, CSS, JavaScript and React and have experience building many applications, architecture,\n",
      "state management, routing and API integrations. I also have a good understanding of data structures\n",
      "and use Git for version control. Based on my research your company emphasizes innovation, collaboration,\n",
      "continuous learning and ownership while building scalable high-quality solutions. I believe I am a good\n",
      "fit for this role because I have strong technical fundamentals, disciplined learning mindset and the\n",
      "ability to adapt quickly. I am motivated to contribute meaningful to the team while continue to grow\n",
      "as a software developer. Thank you.\n",
      "--- END TRANSCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# CELL 1: COMMUNICATION SCORE\n",
    "# ============================\n",
    "\n",
    "audio_path = \"outputs/testing_audio.wav\"\n",
    "\n",
    "cs_out = analyze_interview(audio_path)\n",
    "\n",
    "if cs_out is None:\n",
    "    raise RuntimeError(\"CS analysis failed.\")\n",
    "\n",
    "# ---- Extract only what TCS needs ----\n",
    "transcript = cs_out[\"transcript\"]\n",
    "cs_score = cs_out[\"cs_score\"]\n",
    "\n",
    "print(\"=== COMMUNICATION SCORE (CS) ===\")\n",
    "print(f\"CS Score      : {cs_score}\")\n",
    "print(\"\\n--- TRANSCRIPT ---\")\n",
    "pretty_print_transcript(transcript)\n",
    "print(\"--- END TRANSCRIPT ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf4e2bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TCS] Running technical correctness evaluation...\n",
      "[TCS] Model response received. Parsing output...\n",
      "['Can you tell us about your academic background?', 'What inspired you to pursue a career in software development?', 'How do you stay updated with the latest technologies and trends in the field?', 'Can you walk us through your resume and highlight your relevant skills?', 'What are your greatest strengths and weaknesses as a software developer?', 'Why do you want to work with our company?']\n",
      "\n",
      "=============================================\n",
      "TECHNICAL CORRECTNESS (TCS)\n",
      "=============================================\n",
      "Score : 70\n",
      "Band  : Partial\n",
      "\n",
      "Verdict:\n",
      "The candidate's answer was overly long and meandering, making it difficult to follow their train of thought, and they did not directly address the question.\n",
      "\n",
      "Issues Identified:\n",
      "- The candidate's answer was overly long and meandering, making it difficult to follow their train of thought.\n",
      "- The candidate did not directly answer the question, instead, they provided a detailed description of their skills and interests.\n",
      "- The candidate did not provide specific details about their academic background, such as the university they attended or the specific courses they took.\n",
      "- The candidate did not directly address the question, instead, they provided a sales pitch about why they would be a good fit for the role.\n",
      "\n",
      "=============================================\n",
      "COACHING FEEDBACK\n",
      "=============================================\n",
      "- Encourage the candidate to focus on providing a clear and concise answer to the question, without going off on tangents about their skills and interests.\n",
      "- Ask the candidate to edit their answer to make it more concise and easier to follow.\n",
      "- Ask the candidate to rephrase their answer to directly address the question and provide a clear explanation of their academic background.\n",
      "- Encourage the candidate to prioritize the most important information and cut out unnecessary details.\n",
      "\n",
      "=============================================\n",
      "FINAL INTERVIEW SCORE\n",
      "=============================================\n",
      "82.0\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# CELL 2: TCS + FINAL AGGREGATION\n",
    "# =========================================\n",
    "\n",
    "# ---- Run Technical Correctness Scoring (SINGLE LLM CALL) ----\n",
    "tcs = compute_tcs(transcript ,questions)\n",
    "\n",
    "# ---- Combine CS + TCS ----\n",
    "final_score = combine_cs_tcs(cs_score, tcs)\n",
    "\n",
    "# ---- Pretty Output ----\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"TECHNICAL CORRECTNESS (TCS)\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Score : {tcs.score}\")\n",
    "print(f\"Band  : {tcs.band}\")\n",
    "\n",
    "print(\"\\nVerdict:\")\n",
    "print(tcs.verdict)\n",
    "\n",
    "print(\"\\nIssues Identified:\")\n",
    "for issue in tcs.issues:\n",
    "    print(f\"- {issue}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"COACHING FEEDBACK\")\n",
    "print(\"=\" * 45)\n",
    "for point in tcs.improvement_points:\n",
    "    print(f\"- {point}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"FINAL INTERVIEW SCORE\")\n",
    "print(\"=\" * 45)\n",
    "print(final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9481c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COACH] Running placement coaching...\n",
      "[TCS] Model response received. Parsing output...\n",
      "\n",
      "=============================================\n",
      "RAW COACHING OBJECT\n",
      "=============================================\n",
      "{'standout_strengths': ['strong technical fundamentals', 'disciplined learning mindset', 'ability to adapt quickly', 'problem-solving skills'], 'top_improvements': ['lack of specific tools and technologies mentioned', 'limited experience with back-end development', 'inadequate emphasis on soft skills', 'insufficient detail on project experience'], 'placement_coaching': {'current_gaps': ['back-end development skills', 'soft skills development'], 'actionable_improvements': ['Research and list 3-5 relevant tools and technologies', 'Highlight 2-3 relevant soft skills and provide examples'], 'placement_focus': ['Technical skills assessment', 'Soft skills development and assessment']}}\n",
      "\n",
      "=============================================\n",
      "STANDOUT STRENGTHS\n",
      "=============================================\n",
      "- strong technical fundamentals\n",
      "- disciplined learning mindset\n",
      "- ability to adapt quickly\n",
      "- problem-solving skills\n",
      "\n",
      "=============================================\n",
      "TOP IMPROVEMENTS\n",
      "=============================================\n",
      "- lack of specific tools and technologies mentioned\n",
      "- limited experience with back-end development\n",
      "- inadequate emphasis on soft skills\n",
      "- insufficient detail on project experience\n",
      "\n",
      "=============================================\n",
      "PLACEMENT COACHING INSIGHTS\n",
      "=============================================\n",
      "\n",
      "Where the candidate currently lags:\n",
      "- back-end development skills\n",
      "- soft skills development\n",
      "\n",
      "What should be improved next:\n",
      "- Research and list 3-5 relevant tools and technologies\n",
      "- Highlight 2-3 relevant soft skills and provide examples\n",
      "\n",
      "Areas to focus for placements:\n",
      "- Technical skills assessment\n",
      "- Soft skills development and assessment\n"
     ]
    }
   ],
   "source": [
    "# ---- Placement Coaching ----\n",
    "coaching = generate_placement_feedback(transcript, questions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"RAW COACHING OBJECT\")\n",
    "print(\"=\" * 45)\n",
    "print(coaching)\n",
    "\n",
    "# ---- Standout Strengths ----\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"STANDOUT STRENGTHS\")\n",
    "print(\"=\" * 45)\n",
    "for s in coaching[\"standout_strengths\"]:\n",
    "    print(f\"- {s}\")\n",
    "\n",
    "# ---- Top Improvements ----\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"TOP IMPROVEMENTS\")\n",
    "print(\"=\" * 45)\n",
    "for i in coaching[\"top_improvements\"]:\n",
    "    print(f\"- {i}\")\n",
    "\n",
    "# ---- Placement Coaching Insights ----\n",
    "placement = coaching[\"placement_coaching\"]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(\"PLACEMENT COACHING INSIGHTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\nWhere the candidate currently lags:\")\n",
    "for g in placement[\"current_gaps\"]:\n",
    "    print(f\"- {g}\")\n",
    "\n",
    "print(\"\\nWhat should be improved next:\")\n",
    "for a in placement[\"actionable_improvements\"]:\n",
    "    print(f\"- {a}\")\n",
    "\n",
    "print(\"\\nAreas to focus for placements:\")\n",
    "for f in placement[\"placement_focus\"]:\n",
    "    print(f\"- {f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AItwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
